{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zishan-Shao/s-step-elasticnet-cd/blob/main/s_step_logistic_cd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "disQ6-q49Fld",
        "outputId": "644ed38d-820e-4319-efe7-093f38dfda60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/s-step-Logistic-Coordinate-Descent/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foRxaEkHaeXg",
        "outputId": "8ee04ad7-892b-41cb-ec86-4f07dc11a759"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/s-step-Logistic-Coordinate-Descent/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\forall l\\in \\{1,\\ldots,s\\}$\n",
        "\n",
        "$\\Delta \\beta_j^{k+l} = \\frac{y^T X e_j^{k+l} - e_j^{k+l} X^T \\text{sig}(X\\beta^k) - \\sum_{i=1}^{l-1} e_j^{k+l} X^T \\text{sig}(X\\beta^k) \\odot (1 - \\text{sig}(X\\beta^k)) \\odot X e_j^{k+i} \\Delta \\beta_j^{k+i}}\n",
        "{e_j^{k+l} X^T \\text{sig}(X\\beta^k) \\odot (1 - \\text{sig}(X\\beta^k)) \\odot X e_j^{k+l}},$\n",
        "\n",
        "where $sig(x)$ is a sigmoid function.\n",
        "\n",
        "$\\textbf{Algorithm:}$ \\\\\n",
        "选定一次交流是s.\n",
        "\n",
        "\n",
        "sample s features from p features, each correspond to a basic base($e_{j}^{k+l}$ 表示第$l$个抽取的feature位置为1其余为0的基).\n",
        "\n",
        "$\n",
        "1_s = (e_j^k, \\ldots, e_j^{k+s}) \\in \\mathbb{R}^{p \\times (s+1)} \\\\\n",
        "X 1_s = (X e_j^k, \\ldots, X e_j^{k+s}) \\in \\mathbb{R}^{n \\times (s+1)} \\\\\n",
        "\\text{sig}(X\\beta^k) \\in \\mathbb{R}^{n \\times 1}$\n",
        "\n",
        "$\n",
        "W = \\text{sig}(X\\beta^k) \\odot (1 - \\text{sig}(X\\beta^k)) \\in \\mathbb{R}^{n \\times 1} \\\\\n",
        "$\n",
        "$\n",
        "A = (X 1_s)^T \\text{sig}(X\\beta^k)_{(s+1) \\times 1} \\quad \\Rightarrow \\quad A_{l} = e_j^{k+l} X^T \\text{sig}(X\\beta^k)\n",
        "$\n",
        "$\n",
        "B = (X 1_s)^T (W \\odot (X 1_s)) \\in \\mathbb{R}^{(s+1) \\times (s+1)} \\quad\n",
        "$\n",
        "\n",
        "具体来说$B$中, $W \\odot (X 1_s)$需要把$W$重复(广播)为$(W,W,\\dots,W)_{n\\times s}$\n",
        "\n",
        "$\\Rightarrow B_{l,i} = e_j^{k+l} X^T \\text{sig}(X\\beta^k) \\odot (1 - \\text{sig}(X\\beta^k)) \\odot X e_j^{k+i}\n",
        "$\n",
        "\n",
        "for $l$ in $s$:\n",
        "\n",
        "$\\quad\n",
        "\\Delta \\beta_j^{k+l} = \\frac{y^T X e_j^{k+l} - A_{l} - \\sum_{i=0}^{l-1} B_{l,i} \\Delta \\beta_j^{k+i}}{B_{l,l}}\n",
        "$\n",
        "\n",
        "$\\beta_j^{k+s} = \\beta_{j}^{k}+\\sum_{i=0}^{s-1}\\Delta \\beta_j^{k+i}$"
      ],
      "metadata": {
        "id": "U2UEqTBPpYDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这个部分很重要"
      ],
      "metadata": {
        "id": "LvdipY6YHd-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "from scipy.special import expit  # sigmoid\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def logistic_gradient_descent(X, y, beta, beta_0, s, max_iter=2000, tol= 1e-4, lambda_para=0.2, alpha_para = 0.5):\n",
        "    \"\"\"\n",
        "    Logistic Gradient Descent Algorithm with s-step updates.\n",
        "\n",
        "    Parameters:\n",
        "    X : np.ndarray\n",
        "        Feature matrix (n x p).\n",
        "    y : np.ndarray\n",
        "        Target vector (n x 1).\n",
        "    beta : np.ndarray\n",
        "        Initial coefficients (p x 1).\n",
        "    s : int\n",
        "        Number of features to sample and update at each step.\n",
        "    max_iter : int\n",
        "        Maximum number of iterations.\n",
        "    tol : float\n",
        "        Tolerance for convergence.\n",
        "\n",
        "    Returns:\n",
        "    beta : np.ndarray\n",
        "        Optimized coefficients.\n",
        "    \"\"\"\n",
        "    # n = 100\n",
        "    # p = 10\n",
        "    # s = 5\n",
        "    n, p = X.shape\n",
        "    epsilon = 1e-8\n",
        "    iter = 0\n",
        "    # 每一轮其实是循环大致p/s次\n",
        "    for iteration in range(max_iter*round(p/s + 1)):\n",
        "        # # Sample s features from p features\n",
        "        # sampled_indices = np.random.choice(p, s, replace=True)\n",
        "        start = (iteration * s) % p\n",
        "        end = start + s\n",
        "        # print(end)\n",
        "        flag = 0\n",
        "\n",
        "        # 根据范围选择特征\n",
        "        if end < p:\n",
        "            flag = 0\n",
        "            sampled_indices = np.arange(start, end)  # 在范围内直接取\n",
        "        elif end == p: # this implies that the iteration p has been done\n",
        "            flag = 1\n",
        "            iter += 1\n",
        "            print(iter)\n",
        "            sampled_indices = np.arange(start, end)  # 在范围内直接取\n",
        "        else:\n",
        "            # 超出范围时取余（循环选择）\n",
        "            flag = 1\n",
        "            iter += 1\n",
        "            sampled_indices = np.concatenate((np.arange(start, p), np.arange(0, end % p)))\n",
        "        # print(sampled_indices)\n",
        "        # Construct 1_s and X_{1_s}\n",
        "        one_s = np.eye(p)[:, sampled_indices]  # (p x s)\n",
        "        #print(one_s)\n",
        "\n",
        "        X_1s = X @ one_s  # (n x s)\n",
        "        # print(X_1s)\n",
        "\n",
        "        #if iteration == 0:\n",
        "        #   beta_old = 0  # (n x 1)\n",
        "        # 初始化beta_old\n",
        "        if iteration == 0:\n",
        "          beta_old = beta.copy()\n",
        "\n",
        "        if flag == 1:\n",
        "        # 说明已经循环完一轮了\n",
        "          # print(flag)\n",
        "          print(np.linalg.norm(beta_old - beta))\n",
        "          if np.linalg.norm(beta_old - beta) < tol*np.sqrt(p):\n",
        "            print(f\"ending iteration:{iter}\")\n",
        "            break\n",
        "          # update beta_old\n",
        "          beta_old = beta.copy()\n",
        "\n",
        "        # Compute sigmoid\n",
        "        sigmoid_X_beta = expit(beta_0 + X @ beta)  # (n x 1)\n",
        "        # print(\"beta_0:\", beta_0)\n",
        "        # print(\"sigmoid_X_beta\",sigmoid_X_beta)\n",
        "        # # Check convergence\n",
        "        # if np.linalg.norm(beta - beta_old) < tol and iteration > 2:\n",
        "        #     print(f\"Converged in {iteration + 1} iterations.\")\n",
        "        #     break\n",
        "\n",
        "\n",
        "        # Compute W\n",
        "        W = sigmoid_X_beta * (1 - sigmoid_X_beta)  # (n x 1)\n",
        "        # print(W.shape)\n",
        "        # print(\"W\",W)\n",
        "        delta_beta_0 = (np.sum(y)-np.sum(sigmoid_X_beta))/(np.sum(W)+epsilon)\n",
        "\n",
        "        # if np.abs(delta_beta_0)<= np.abs(beta_0):\n",
        "\n",
        "        beta_0 += delta_beta_0\n",
        "\n",
        "        # Compute A\n",
        "        A = (X_1s.T @ sigmoid_X_beta)  # (s x 1)\n",
        "        #print(A.shape)\n",
        "\n",
        "        # Compute B\n",
        "        B = (X_1s.T @ (np.tile(W, (1, s)) * X_1s))  # (s x s)\n",
        "        #print(B.shape)\n",
        "\n",
        "        # Update beta for each feature in s\n",
        "        delta_beta = np.zeros((p, 1))\n",
        "        delta_beta_elastic = np.zeros((p, 1))\n",
        "        for l in range(s):\n",
        "            B_l_l = B[l, l]\n",
        "            A_l = A[l]\n",
        "\n",
        "            summation_term = 0\n",
        "            for i in range(l):\n",
        "                B_l_i = B[l, i]\n",
        "                summation_term += B_l_i * delta_beta_elastic[sampled_indices[i]] #这个地方改了\n",
        "            #print(summation_term)\n",
        "\n",
        "\n",
        "            # 弹性网\n",
        "            delta_beta_l = ((1/n)*(y.T @ X_1s[:, l] - A_l - summation_term)-lambda_para*(1-alpha_para)*beta[sampled_indices[l]]) / ((1/n)*B_l_l + lambda_para*(1-alpha_para)+epsilon)\n",
        "            # delta_beta_l = ((1/n)*(y.T @ X_1s[:, l] - A_l - summation_term)-lambda_para*beta[sampled_indices[l]]) / ((1/n)*B_l_l + lambda_para)\n",
        "\n",
        "            #print(f\"delta_beta_l:{delta_beta_l}\")\n",
        "            # delta_beta[sampled_indices[l]] = delta_beta_l\n",
        "            # beta[sampled_indices[l]] += delta_beta_l\n",
        "\n",
        "            # 弹性网\n",
        "            # print(delta_beta_l)\n",
        "            # print(lambda_para*alpha_para/ ((1/n)*B_l_l + lambda_para*(1-alpha_para)))\n",
        "            # print(\"*\"*80)\n",
        "            l1_term = lambda_para*alpha_para/ ((1/n)*B_l_l + lambda_para*(1-alpha_para)+epsilon)\n",
        "            # if delta_beta_l < 0:\n",
        "            #     print(f\"less than 0{delta_beta_l}\")\n",
        "            #     print(f\"{beta[sampled_indices[l]]}\")\n",
        "            #     print(f\"l1_term:{l1_term}\")\n",
        "            delta_beta_elastic_l = 0\n",
        "\n",
        "            if np.abs(beta[sampled_indices[l]] + delta_beta_l) <= l1_term:\n",
        "                delta_beta_elastic_l = beta[sampled_indices[l]]\n",
        "                beta[sampled_indices[l]] = 0\n",
        "                # print(f\"type 1:{beta[sampled_indices[l]]}\")\n",
        "            elif beta[sampled_indices[l]] + delta_beta_l < 0:\n",
        "                delta_beta_elastic_l = delta_beta_l + l1_term\n",
        "                beta[sampled_indices[l]] += delta_beta_elastic_l\n",
        "\n",
        "                # print(f\"type 2:{beta[sampled_indices[l]]}\")\n",
        "            else:\n",
        "                delta_beta_elastic_l = delta_beta_l - l1_term\n",
        "                beta[sampled_indices[l]] += delta_beta_elastic_l\n",
        "               # print(f\"type 3:{beta[sampled_indices[l]]}\")\n",
        "            # print(delta_beta_elastic_l )\n",
        "            delta_beta_elastic[sampled_indices[l]] = delta_beta_elastic_l\n",
        "\n",
        "\n",
        "        #print(beta)\n",
        "        # # Check convergence\n",
        "        # if np.linalg.norm(delta_beta_elastic) < tol:\n",
        "        #     print(f\"Converged in {iteration + 1} iterations.\")\n",
        "        #     print(np.count_nonzero(delta_beta_elastic))\n",
        "\n",
        "        #     break\n",
        "\n",
        "    return beta, beta_0\n"
      ],
      "metadata": {
        "id": "nAYUj6-VtTiO"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "读数据"
      ],
      "metadata": {
        "id": "S1FuXoK0IV4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 定义文件路径\n",
        "file_path = \"/content/drive/My Drive/s-step-Logistic-Coordinate-Descent/data/colon-cancer.txt\"\n",
        "\n",
        "# 初始化存储\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# 解析文件内容d\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        # 分割每一行数据\n",
        "        parts = line.strip().split()\n",
        "        # 提取目标变量 y（第一列），将浮点数转换为整数\n",
        "        y.append(int(float(parts[0])))\n",
        "        # 提取特征并存入字典，键为特征索引，值为特征值\n",
        "        features = {int(kv.split(':')[0]): float(kv.split(':')[1]) for kv in parts[1:]}\n",
        "        X.append(features)\n",
        "\n",
        "# 转换为 Pandas DataFrame，填充缺失值为 0\n",
        "X = pd.DataFrame(X).fillna(0).sort_index(axis=1)  # 按列排序\n",
        "\n",
        "# 将列名转换为字符串（保持一致性）\n",
        "X.columns = X.columns.astype(str)\n",
        "\n",
        "# 转换目标变量为 NumPy 数组\n",
        "y = np.array(y)\n",
        "\n",
        "# 将目标变量从 -1 和 1 转换为 0 和 1\n",
        "y = np.where(y == -1, 0, y)\n",
        "\n",
        "# 检查结果\n",
        "print(\"Feature matrix X (first 5 rows):\")\n",
        "print(X.head())\n",
        "print(\"\\nTarget variable y (first 5 values):\")\n",
        "print(y[:5])\n",
        "\n",
        "# 保存特征矩阵 X 和目标变量 y 到文件（如果需要）\n",
        "# X.to_csv(\"X.csv\", index=False)\n",
        "# np.savetxt(\"y.csv\", y, delimiter=\",\", fmt=\"%d\")\n",
        "\n",
        "# 完成\n",
        "print(\"\\nData processing complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7-MFR7F6PNF",
        "outputId": "090cdae4-b680-4e70-ef0e-b82dcf65550f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix X (first 5 rows):\n",
            "          1         2         3         4         5         6         7  \\\n",
            "0  2.080750  1.099070  0.927763  1.029080 -0.130763  1.265460 -0.436286   \n",
            "1  1.109460  0.786453  0.445560 -0.146323 -0.996316  0.555759  0.290734   \n",
            "2 -0.676530  1.693100  1.559250  1.559980 -0.982179 -1.358510 -1.313990   \n",
            "3  0.534396  1.677540  1.489030  0.778605 -0.183776 -1.116850 -1.487560   \n",
            "4 -1.018900  0.511080  0.755641  1.013820  0.529899  0.160440 -0.087055   \n",
            "\n",
            "          8         9        10  ...      1991      1992      1993      1994  \\\n",
            "0  0.728881  2.107980  1.359870  ... -0.825403 -0.138451  0.382957  0.876697   \n",
            "1 -0.145259  1.132660  0.559093  ... -1.056290 -0.205499 -1.815370  0.324373   \n",
            "2 -0.455067  0.295214  0.290694  ...  1.242970  1.230160 -2.039000  2.366090   \n",
            "3 -0.579511  0.292683  1.345480  ...  0.559852 -0.593149 -4.440580  1.720710   \n",
            "4  1.295290  0.458736  0.714082  ...  0.227110  0.497628 -0.083921 -0.382733   \n",
            "\n",
            "       1995      1996      1997      1998      1999      2000  \n",
            "0 -0.216234 -1.408300  0.393327 -0.148522  1.591530 -0.217481  \n",
            "1 -1.296910 -0.870757  1.108740  1.094010 -0.492141 -1.554080  \n",
            "2  0.820656  1.404500  0.176860 -0.086285 -0.390878 -0.089465  \n",
            "3 -0.124617 -0.435880  0.228440 -0.893938  1.005880 -0.631247  \n",
            "4 -0.913389  1.122930  0.834571 -0.283786 -2.860340  0.280871  \n",
            "\n",
            "[5 rows x 2000 columns]\n",
            "\n",
            "Target variable y (first 5 values):\n",
            "[0 1 0 1 0]\n",
            "\n",
            "Data processing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "np.random.seed(42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# 假设 X 是特征数据，y 是目标变量\n",
        "# 确保 X 和 y 的顺序一致\n",
        "n_samples = len(X)\n",
        "split_index = int(n_samples * 0.8)  # 按 80% 划分\n",
        "\n",
        "# 按顺序切分数据\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# # Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "n, p = X.shape\n",
        "beta_init = np.zeros((p, 1))\n",
        "beta_0_init = np.log(np.mean(y_train) / (1 - np.mean(y_train)))\n",
        "s = 1024\n",
        "\n",
        "\n",
        "# Measure time for coordinate gradient descent\n",
        "start_time_cg = time.time()\n",
        "beta, beta_0 = logistic_gradient_descent(X_train, y_train, beta_init,beta_0_init, s)\n",
        "y_pred_cg = expit(beta_0 + X_test @ beta) >= 0.5\n",
        "accuracy_cg = accuracy_score(y_test, y_pred_cg)\n",
        "cg_time = time.time() - start_time_cg\n",
        "print(f\"Coordinate Gradient Descent beta:{beta_0},{beta}\")\n",
        "print(f\"Coordinate Gradient Descent Accuracy: {accuracy_cg:.4f}\")\n",
        "print(f\"Coordinate Gradient Descent Time: {cg_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "# # Measure time for coordinate gradient descent\n",
        "# start_time_cg = time.time()\n",
        "# beta, beta_0 = logistic_gradient_descent(X_train, y_train, beta_init,beta_0_init, s=1)\n",
        "# y_pred_cg = expit(beta_0 + X_test @ beta) >= 0.5\n",
        "# accuracy_cg = accuracy_score(y_test, y_pred_cg)\n",
        "# cg_time = time.time() - start_time_cg\n",
        "# #print(f\"Coordinate Gradient Descent bera:{beta}\")\n",
        "# print(f\"Coordinate Gradient Descent Accuracy: {accuracy_cg:.4f}\")\n",
        "# print(f\"Coordinate Gradient Descent Time: {cg_time:.4f} seconds\")\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import time\n",
        "\n",
        "# 定义超参数\n",
        "alpha = 0.2\n",
        "l1_ratio = 0.5\n",
        "\n",
        "# 开始计时\n",
        "start_time_en = time.time()\n",
        "\n",
        "# 创建 ElasticNet 模型\n",
        "elastic_net_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=2000, random_state=42)\n",
        "elastic_net_model.fit(X_train, y_train)\n",
        "\n",
        "# 进行预测\n",
        "y_pred_en = elastic_net_model.predict(X_test) >= 0.5\n",
        "\n",
        "# 计算准确率\n",
        "accuracy_en = accuracy_score(y_test, y_pred_en)\n",
        "print(f\"ElasticNet Accuracy: {accuracy_en:.4f}\")\n",
        "\n",
        "# 计算性能指标\n",
        "mse = mean_squared_error(y_test, y_pred_en)\n",
        "r2 = r2_score(y_test, y_pred_en)\n",
        "en_time = time.time() - start_time_en\n",
        "\n",
        "# 打印结果\n",
        "accuracy_cgd = accuracy_score(y_test, y_pred_en)\n",
        "\n",
        "print(f\"Coordinate Gradient Descent Accuracy: {accuracy_cgd:.4f}\")\n",
        "print(f\"ElasticNet Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"ElasticNet R2 Score: {r2:.4f}\")\n",
        "print(f\"ElasticNet Time: {en_time:.4f} seconds\")\n",
        "\n",
        "# Measure time for Logistic Regression (default solver)\n",
        "start_time_lr = time.time()\n",
        "logistic_model = LogisticRegression(max_iter=2000, random_state=42)\n",
        "logistic_model.fit(X_train, y_train)\n",
        "y_pred_lr = logistic_model.predict(X_test)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "lr_time = time.time() - start_time_lr\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_lr:.4f}\")\n",
        "print(f\"Logistic Regression Time: {lr_time:.4f} seconds\")\n",
        "\n",
        "# Measure time for Logistic Regression with liblinear solver\n",
        "start_time_lr_liblinear = time.time()\n",
        "# # # L1 正则化 (lambda=0.1)\n",
        "# logistic_model_liblinear = LogisticRegression(\n",
        "#     solver='liblinear',\n",
        "#     penalty='l1',  # 或 'l2'，根据需要选择\n",
        "#     C=2,          # 设置 lambda 的倒数\n",
        "#     max_iter=2000,\n",
        "#     random_state=42\n",
        "# )\n",
        "# logistic_model_liblinear = LogisticRegression(\n",
        "#     solver='saga',\n",
        "#     penalty='elasticnet',  # 指定 Elastic Net 正则化\n",
        "#     l1_ratio=0.5,          # 设置 alpha 参数\n",
        "#     C=5,                  # 设置 lambda 的倒数\n",
        "#     max_iter=2000,\n",
        "#     random_state=42\n",
        "# )\n",
        "logistic_model_liblinear = LogisticRegression(solver='liblinear', max_iter=2000, random_state=42)\n",
        "logistic_model_liblinear.fit(X_train, y_train)\n",
        "y_pred_lr_liblinear = logistic_model_liblinear.predict(X_test)\n",
        "accuracy_lr_liblinear = accuracy_score(y_test, y_pred_lr_liblinear)\n",
        "lr_liblinear_time = time.time() - start_time_lr_liblinear\n",
        "print(f\"Logistic Regression (liblinear) Accuracy: {accuracy_lr_liblinear:.4f}\")\n",
        "print(f\"Logistic Regression (liblinear) Time: {lr_liblinear_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmTtqgxNcOBz",
        "outputId": "98f1bbf0-2fee-42e6-95f1-862f9998530b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7918425545197283\n",
            "0.6588422017577934\n",
            "0.3951664546316623\n",
            "0.3285337046778384\n",
            "0.29493786611430534\n",
            "0.2122478788966817\n",
            "0.17015227429762877\n",
            "0.11001601738650993\n",
            "0.09945463009945861\n",
            "0.09189758938218505\n",
            "0.06361961352770681\n",
            "0.05955981783943549\n",
            "0.052042884871088026\n",
            "0.058690730856820826\n",
            "0.050244410643121074\n",
            "0.04695798156752664\n",
            "0.04101909830864024\n",
            "0.03633168875594555\n",
            "0.032441148699122964\n",
            "0.029012471373662647\n",
            "0.024912464653002987\n",
            "0.017871837723092848\n",
            "0.021336843243015897\n",
            "0.019143479995714422\n",
            "0.016337904986377095\n",
            "0.015934316012337187\n",
            "0.014680647430076806\n",
            "0.013156665599788472\n",
            "0.011170403005435859\n",
            "0.010015479368120394\n",
            "0.009090837147805569\n",
            "0.008158886754348012\n",
            "0.007199916418502313\n",
            "0.005993517390465304\n",
            "0.0050861099643707015\n",
            "0.006263433201963035\n",
            "0.00419496069160578\n",
            "ending iteration:37\n",
            "Coordinate Gradient Descent beta:-1.0394172315260441,[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " ...\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Coordinate Gradient Descent Accuracy: 0.6923\n",
            "Coordinate Gradient Descent Time: 174.8882 seconds\n",
            "ElasticNet Accuracy: 0.6923\n",
            "Coordinate Gradient Descent Accuracy: 0.6923\n",
            "ElasticNet Mean Squared Error: 0.3077\n",
            "ElasticNet R2 Score: -0.2381\n",
            "ElasticNet Time: 0.0201 seconds\n",
            "Logistic Regression Accuracy: 0.6923\n",
            "Logistic Regression Time: 0.0232 seconds\n",
            "Logistic Regression (liblinear) Accuracy: 0.6923\n",
            "Logistic Regression (liblinear) Time: 0.0291 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not use python's glmnet as baseline because it is not accurate, so we implement a python-based\n",
        "\n",
        "Adaptive Learning rate: with closer to the solution, we make the learning rate smaller so less likely to fall into"
      ],
      "metadata": {
        "id": "fMl0II27dDt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Optimization & Baseline"
      ],
      "metadata": {
        "id": "Kpo5MDzLdbID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: s-step code optimization\n",
        "\n",
        "first one is with sparsification, but not well on colon-cancer dataset, which is dense"
      ],
      "metadata": {
        "id": "MB6ug2rWddve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "from scipy.special import expit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def logistic_gradient_descent_sparse(X, y, beta, beta_0, s, max_iter=2000, tol=1e-4,\n",
        "                                     lambda_para=0.2, alpha_para=0.5):\n",
        "    \"\"\"\n",
        "    Logistic Gradient Descent Algorithm with s-step updates (sparsified).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : np.ndarray or scipy.sparse.csr_matrix\n",
        "        Feature matrix (n x p).\n",
        "    y : np.ndarray\n",
        "        Target vector (n,). Values should be 0 or 1.\n",
        "    beta : np.ndarray\n",
        "        Initial coefficients of shape (p, 1) or (p,).\n",
        "    beta_0 : float\n",
        "        Initial intercept term.\n",
        "    s : int\n",
        "        Number of features to sample and update at each step.\n",
        "    max_iter : int\n",
        "        Maximum number of iterations (each iteration updates `s` features).\n",
        "    tol : float\n",
        "        Tolerance for convergence based on L2 norm of coefficient change.\n",
        "    lambda_para : float\n",
        "        Regularization strength for elastic net.\n",
        "    alpha_para : float\n",
        "        Mixing parameter for elastic net. alpha=1 => L1, alpha=0 => L2.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    beta : np.ndarray\n",
        "        Optimized coefficients (shape same as input).\n",
        "    beta_0 : float\n",
        "        Optimized intercept term.\n",
        "    \"\"\"\n",
        "    # Ensure X is in csr_matrix format\n",
        "    if not isinstance(X, csr_matrix):\n",
        "        X = csr_matrix(X)\n",
        "\n",
        "    n, p = X.shape\n",
        "    epsilon = 1e-8\n",
        "    iteration_count = 0\n",
        "\n",
        "    # If beta is (p,1), flatten it to (p,) for easier indexing\n",
        "    # but keep in mind we might need to reshape back later\n",
        "    if beta.ndim == 2 and beta.shape[1] == 1:\n",
        "        beta = beta.flatten()\n",
        "\n",
        "    beta_old = beta.copy()\n",
        "\n",
        "    # Number of outer loops: each iteration updates 's' features, so we cycle ~p/s times in one pass\n",
        "    max_loops = max_iter * (round(p / s + 1))\n",
        "\n",
        "    for iteration in range(max_loops):\n",
        "\n",
        "        # Determine sampled indices\n",
        "        start = (iteration * s) % p\n",
        "        end = start + s\n",
        "        if end <= p:\n",
        "            sampled_indices = np.arange(start, end)\n",
        "        else:\n",
        "            # Wrap around\n",
        "            sampled_indices = np.concatenate(\n",
        "                (np.arange(start, p), np.arange(0, end % p))\n",
        "            )\n",
        "\n",
        "        # Construct X_1s for selected features\n",
        "        # Create a sparse identity-like submatrix with ones on [sampled_indices, range(s)]\n",
        "        one_s = csr_matrix(\n",
        "            (np.ones(len(sampled_indices)),\n",
        "             (sampled_indices, np.arange(len(sampled_indices)))),\n",
        "            shape=(p, len(sampled_indices))\n",
        "        )\n",
        "        X_1s = X @ one_s  # shape (n, s)\n",
        "\n",
        "        # Check for convergence each time we hit a multiple of p features\n",
        "        # i.e., after ~one full pass of p features\n",
        "        if end % p == 0 and iteration != 0:\n",
        "            iteration_count += 1\n",
        "            #print(f\"Epoch {iteration_count}: loss {np.linalg.norm(beta_old - beta)}.\")\n",
        "            #print(np.linalg.norm(beta_old - beta))\n",
        "            diff_norm = np.linalg.norm(beta_old - beta)\n",
        "            if diff_norm < tol * np.sqrt(p):\n",
        "                print(f\"Converged at iteration {iteration_count}\")\n",
        "                break\n",
        "            beta_old = beta.copy()\n",
        "\n",
        "\n",
        "\n",
        "        # Compute sigmoid\n",
        "        # X @ beta => shape (n,). Make sure shapes align.\n",
        "        logits = beta_0 + (X @ beta)\n",
        "        sigmoid_X_beta = expit(logits)\n",
        "\n",
        "        # Compute W diagonal (as a 1D array)\n",
        "        W = (sigmoid_X_beta * (1 - sigmoid_X_beta)).flatten()\n",
        "\n",
        "        # Construct diagonal weight matrix in sparse form\n",
        "        W_sparse = diags(W, 0, shape=(n, n), format='csr')\n",
        "\n",
        "        # Update beta_0\n",
        "        # sum of residuals over sum of W\n",
        "        delta_beta_0 = (np.sum(y) - np.sum(sigmoid_X_beta)) / (np.sum(W) + epsilon)\n",
        "        beta_0 += delta_beta_0\n",
        "\n",
        "        # Compute A and B\n",
        "        # A = X_1s^T * sigmoid_X_beta\n",
        "        A = X_1s.T @ sigmoid_X_beta  # shape (s,)\n",
        "        # B = X_1s^T * W_sparse * X_1s\n",
        "        B = X_1s.T @ (W_sparse @ X_1s)  # shape (s,s)\n",
        "\n",
        "        # Update beta for sampled features\n",
        "        for l in range(len(sampled_indices)):\n",
        "            idx = sampled_indices[l]\n",
        "\n",
        "            B_l_l = B[l, l]\n",
        "            A_l = A[l]\n",
        "\n",
        "            # summation_term = B[l, :l] @ beta for indices in sampled_indices[:l]\n",
        "            # Safely convert partial row to dense if needed\n",
        "            B_l_range = B[l, :l]\n",
        "            if hasattr(B_l_range, \"toarray\"):\n",
        "                B_l_range = B_l_range.toarray().ravel()  # flatten to 1D\n",
        "\n",
        "            summation_term = B_l_range @ beta[sampled_indices[:l]]\n",
        "\n",
        "            # Numerator for coordinate descent\n",
        "            numerator = ((1.0 / n) * (y @ X_1s[:, l] - A_l - summation_term)\n",
        "                         - lambda_para * (1 - alpha_para) * beta[idx])\n",
        "\n",
        "            denominator = ((1.0 / n) * B_l_l\n",
        "                           + lambda_para * (1 - alpha_para)\n",
        "                           + epsilon)\n",
        "\n",
        "            delta_beta_l = numerator / denominator\n",
        "\n",
        "            # L1 shrinkage (soft thresholding)\n",
        "            l1_term = lambda_para * alpha_para / denominator\n",
        "            current_beta = beta[idx] + delta_beta_l\n",
        "\n",
        "            #if np.abs(current_beta) <= l1_term:\n",
        "            #    beta[idx] = 0.0\n",
        "            #elif current_beta < 0:\n",
        "            #    beta[idx] = current_beta + l1_term\n",
        "            #else:\n",
        "            #    beta[idx] = current_beta - l1_term\n",
        "            if np.abs(current_beta) <= l1_term:\n",
        "                beta[idx] = 0.0\n",
        "            elif current_beta < 0:\n",
        "                beta[idx] = float(current_beta + l1_term)  # or .item()\n",
        "            else:\n",
        "                beta[idx] = float(current_beta - l1_term)  # or .item()\n",
        "\n",
        "\n",
        "    # Reshape beta back to (p,1) if needed\n",
        "    if beta.ndim == 1:\n",
        "        beta = beta.reshape(-1, 1)\n",
        "\n",
        "    return beta, beta_0\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                         TEST SCRIPT BELOW\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Data Loading and Preprocessing\n",
        "file_path = \"/content/drive/My Drive/s-step-Logistic-Coordinate-Descent/data/colon-cancer.txt\"\n",
        "X_list, y_list = [], []\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        # Convert label -1 to 0\n",
        "        label = int(float(parts[0]))\n",
        "        y_list.append(0 if label == -1 else label)\n",
        "        features = {\n",
        "            int(kv.split(':')[0]): float(kv.split(':')[1])\n",
        "            for kv in parts[1:]\n",
        "        }\n",
        "        X_list.append(features)\n",
        "\n",
        "X = pd.DataFrame(X_list).fillna(0).sort_index(axis=1)  # shape (n, p)\n",
        "y = np.array(y_list)  # shape (n,)\n",
        "\n",
        "print('Data Loading Complete')\n",
        "\n",
        "# 2. Train / Test Split\n",
        "n_samples = len(X)\n",
        "split_index = int(n_samples * 0.8)\n",
        "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# 3. Standardize Features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert to Sparse (optional, beneficial if data is truly sparse)\n",
        "X_train_csr = csr_matrix(X_train)\n",
        "X_test_csr = csr_matrix(X_test)\n",
        "\n",
        "# 5. Initialize parameters for logistic gradient descent\n",
        "n, p = X.shape\n",
        "beta_init = np.zeros((p, 1))\n",
        "beta_0_init = np.log(np.mean(y_train) / (1 - np.mean(y_train) + 1e-12))  # avoid zero\n",
        "s = 1024  # number of features updated at once\n",
        "\n",
        "# 6. Run Custom Logistic Gradient Descent\n",
        "start_time_cg = time.time()\n",
        "beta, beta_0 = logistic_gradient_descent_sparse(\n",
        "    X_train_csr,\n",
        "    y_train,\n",
        "    beta_init,\n",
        "    beta_0_init,\n",
        "    s,\n",
        "    max_iter=2000,\n",
        "    tol=1e-4,\n",
        "    lambda_para=0.2,\n",
        "    alpha_para=0.5\n",
        ")\n",
        "cg_time = time.time() - start_time_cg\n",
        "\n",
        "# 7. Evaluate Performance\n",
        "# beta should be shape (p,1). Do X_test @ beta => shape (n_test,)\n",
        "logits_test = beta_0 + X_test_csr @ beta\n",
        "y_pred_cg = expit(logits_test).ravel() >= 0.5\n",
        "accuracy_cg = accuracy_score(y_test, y_pred_cg)\n",
        "\n",
        "print(f\"Custom Gradient Descent Accuracy: {accuracy_cg:.4f}, Time: {cg_time:.4f}s\")\n",
        "\n",
        "# 8. Compare with scikit-learn’s LogisticRegression\n",
        "start_time_lr = time.time()\n",
        "logistic_model = LogisticRegression(max_iter=2000, random_state=42)\n",
        "logistic_model.fit(X_train_csr, y_train)\n",
        "lr_time = time.time() - start_time_lr\n",
        "\n",
        "y_pred_lr = logistic_model.predict(X_test_csr)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"Logistic Regression (default solver) Accuracy: {accuracy_lr:.4f}, Time: {lr_time:.4f}s\")\n",
        "\n",
        "start_time_lr_liblinear = time.time()\n",
        "logistic_model_liblinear = LogisticRegression(solver='liblinear', max_iter=2000, random_state=42)\n",
        "logistic_model_liblinear.fit(X_train_csr, y_train)\n",
        "lr_liblinear_time = time.time() - start_time_lr_liblinear\n",
        "\n",
        "y_pred_lr_liblinear = logistic_model_liblinear.predict(X_test_csr)\n",
        "accuracy_lr_liblinear = accuracy_score(y_test, y_pred_lr_liblinear)\n",
        "\n",
        "print(f\"Logistic Regression (liblinear) Accuracy: {accuracy_lr_liblinear:.4f}, Time: {lr_liblinear_time:.4f}s\")\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n--- Summary ---\")\n",
        "print(f\"Custom Logistic CD Accuracy: {accuracy_cg:.4f} | Time: {cg_time:.4f}s\")\n",
        "print(f\"Logistic Regression (default) Accuracy: {accuracy_lr:.4f} | Time: {lr_time:.4f}s\")\n",
        "print(f\"Logistic Regression (liblinear) Accuracy: {accuracy_lr_liblinear:.4f} | Time: {lr_liblinear_time:.4f}s\")\n"
      ],
      "metadata": {
        "id": "j8JY7dvrcNUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from scipy.special import expit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "###############################################################################\n",
        "# Vectorized Logistic Coordinate Descent Function\n",
        "###############################################################################\n",
        "def logistic_cd_fast(\n",
        "    X, y, beta, beta_0, s, max_iter=2000, tol=1e-4,\n",
        "    lambda_para=0.2, alpha_para=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Logistic Coordinate Descent with s-step block updates (elastic net regularization).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.ndarray of shape (n, p)\n",
        "        Feature matrix.\n",
        "    y : np.ndarray of shape (n,)\n",
        "        Binary target vector (0/1).\n",
        "    beta : np.ndarray of shape (p, 1) or (p,)\n",
        "        Initial coefficients.\n",
        "    beta_0 : float\n",
        "        Initial intercept.\n",
        "    s : int\n",
        "        Number of features to update at a time.\n",
        "    max_iter : int\n",
        "        Maximum number of epochs (full passes).\n",
        "    tol : float\n",
        "        Convergence threshold based on L2 norm of coefficient change.\n",
        "    lambda_para : float\n",
        "        Regularization strength for elastic net.\n",
        "    alpha_para : float\n",
        "        Mixing parameter for elastic net (0 = L2, 1 = L1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    beta : np.ndarray of shape (p, 1)\n",
        "        Learned coefficients.\n",
        "    beta_0 : float\n",
        "        Learned intercept.\n",
        "    \"\"\"\n",
        "\n",
        "    # Flatten beta to shape (p,) for easier indexing\n",
        "    if beta.ndim == 2 and beta.shape[1] == 1:\n",
        "        beta = beta.flatten()\n",
        "\n",
        "    n, p = X.shape\n",
        "    epsilon = 1e-8\n",
        "    epoch = 0\n",
        "    beta_old = beta.copy()\n",
        "\n",
        "    # We'll do max_iter passes, each pass ~ p/s block-updates\n",
        "    total_iter = max_iter * int(np.ceil(p / s))\n",
        "\n",
        "    # Helper to compute logistic loss w/ elastic net penalty\n",
        "    def logistic_loss(y_true, y_prob, coef):\n",
        "        eps = 1e-8\n",
        "        # Negative log-likelihood\n",
        "        neg_ll = -(\n",
        "            y_true @ np.log(y_prob + eps) +\n",
        "            (1 - y_true) @ np.log(1 - y_prob + eps)\n",
        "        ) / n\n",
        "        # Elastic net penalty\n",
        "        l2_term = 0.5 * np.sum(coef**2)\n",
        "        l1_term = np.sum(np.abs(coef))\n",
        "        penalty = lambda_para * ((1 - alpha_para) * l2_term + alpha_para * l1_term)\n",
        "        return neg_ll + penalty\n",
        "\n",
        "    for iteration in range(total_iter):\n",
        "        start = (iteration * s) % p\n",
        "        end = start + s\n",
        "        if end <= p:\n",
        "            sampled_indices = np.arange(start, end)\n",
        "        else:\n",
        "            # Wrap around if needed\n",
        "            sampled_indices = np.concatenate([\n",
        "                np.arange(start, p),\n",
        "                np.arange(0, end % p)\n",
        "            ])\n",
        "\n",
        "        # Check if we've completed one full pass of p features => an \"epoch\"\n",
        "        if end % p == 0 and iteration != 0:\n",
        "            epoch += 1\n",
        "            # Compute current loss and print progress\n",
        "            y_prob = expit(beta_0 + X @ beta)\n",
        "            loss_val = logistic_loss(y, y_prob, beta)\n",
        "            diff_norm = np.linalg.norm(beta_old - beta)\n",
        "            print(f\"Epoch {epoch}, Loss={loss_val:.6f}, ||Δβ||={diff_norm:.6f}\")\n",
        "\n",
        "            # Convergence check\n",
        "            if diff_norm < tol * np.sqrt(p):\n",
        "                print(f\"Converged at epoch {epoch}\")\n",
        "                break\n",
        "            beta_old = beta.copy()\n",
        "\n",
        "        # Compute sigmoid & derivative\n",
        "        sig = expit(beta_0 + X @ beta)\n",
        "        W = sig * (1 - sig)\n",
        "\n",
        "        # Update intercept\n",
        "        delta_beta_0 = (np.sum(y) - np.sum(sig)) / (np.sum(W) + epsilon)\n",
        "        beta_0 += delta_beta_0\n",
        "\n",
        "        # Extract the block X_1s\n",
        "        X_1s = X[:, sampled_indices]\n",
        "\n",
        "        # Compute A and B for the chosen block\n",
        "        A = X_1s.T @ sig\n",
        "        B = X_1s.T @ (W[:, None] * X_1s)\n",
        "\n",
        "        # Update coefficients in this block\n",
        "        for l in range(len(sampled_indices)):\n",
        "            idx = sampled_indices[l]\n",
        "            B_ll = B[l, l]\n",
        "            A_l = A[l]\n",
        "\n",
        "            # Summation of updates from previously updated coords in this block\n",
        "            block_indices = sampled_indices[:l]\n",
        "            summation_term = B[l, :l] @ (beta[block_indices] - beta_old[block_indices])\n",
        "\n",
        "            numerator = (\n",
        "                (1.0 / n)*(y @ X_1s[:, l] - A_l - summation_term)\n",
        "                - lambda_para * (1 - alpha_para) * beta[idx]\n",
        "            )\n",
        "            denominator = ((1.0 / n)*B_ll + lambda_para*(1 - alpha_para) + epsilon)\n",
        "            db_l = numerator / denominator\n",
        "            l1_term = lambda_para * alpha_para / denominator\n",
        "\n",
        "            candidate = beta[idx] + db_l\n",
        "            if abs(candidate) <= l1_term:\n",
        "                beta[idx] = 0.0\n",
        "            elif candidate < 0:\n",
        "                beta[idx] = candidate + l1_term\n",
        "            else:\n",
        "                beta[idx] = candidate - l1_term\n",
        "\n",
        "    # Reshape back to (p,1)\n",
        "    return beta.reshape(-1, 1), beta_0\n",
        "\n",
        "###############################################################################\n",
        "# Test Script (Runs Directly)\n",
        "###############################################################################\n",
        "\n",
        "# 1. Load data (colon-cancer.txt, for example). Update path if needed:\n",
        "file_path = \"/content/drive/My Drive/s-step-Logistic-Coordinate-Descent/data/colon-cancer.txt\"\n",
        "X_list, y_list = [], []\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        label = int(float(parts[0]))\n",
        "        y_list.append(0 if label == -1 else label)\n",
        "        features = {\n",
        "            int(kv.split(':')[0]): float(kv.split(':')[1])\n",
        "            for kv in parts[1:]\n",
        "        }\n",
        "        X_list.append(features)\n",
        "\n",
        "X = pd.DataFrame(X_list).fillna(0).sort_index(axis=1)\n",
        "y = np.array(y_list)\n",
        "print(\"Data loaded. Shape of X:\", X.shape)\n",
        "\n",
        "# 2. Train/Test Split\n",
        "n_samples = X.shape[0]\n",
        "split_index = int(n_samples * 0.8)\n",
        "X_train = X.iloc[:split_index].values\n",
        "y_train = y[:split_index]\n",
        "X_test  = X.iloc[split_index:].values\n",
        "y_test  = y[split_index:]\n",
        "\n",
        "# 3. Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Initialization\n",
        "n, p = X_train.shape\n",
        "beta_init = np.zeros((p, 1))\n",
        "eps = 1e-12\n",
        "y_mean = np.mean(y_train)\n",
        "beta_0_init = np.log((y_mean + eps) / (1 - y_mean + eps))\n",
        "\n",
        "# 5. Run our logistic CD\n",
        "s = 1024\n",
        "max_iter = 200\n",
        "start_time_cd = time.time()\n",
        "beta_cd, beta_0_cd = logistic_cd_fast(\n",
        "    X_train, y_train, beta_init, beta_0_init, s,\n",
        "    max_iter=max_iter, tol=1e-4, lambda_para=0.2, alpha_para=0.5\n",
        ")\n",
        "cd_time = time.time() - start_time_cd\n",
        "\n",
        "# 6. Evaluate Performance\n",
        "logits_test_cd = beta_0_cd + X_test @ beta_cd\n",
        "y_prob_cd = expit(logits_test_cd).ravel()\n",
        "y_pred_cd = (y_prob_cd >= 0.5).astype(int)\n",
        "acc_cd = accuracy_score(y_test, y_pred_cd)\n",
        "\n",
        "print(f\"\\nCustom Logistic CD => Accuracy: {acc_cd:.4f}, Time: {cd_time:.2f}s\")\n",
        "\n",
        "# 7. Compare with scikit-learn LogisticRegression\n",
        "#lr_start = time.time()\n",
        "#lr_model = LogisticRegression(max_iter=2000, random_state=42)\n",
        "#lr_model.fit(X_train, y_train)\n",
        "#lr_time = time.time() - lr_start\n",
        "\n",
        "#y_pred_lr = lr_model.predict(X_test)\n",
        "#acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "\n",
        "#print(f\"Sklearn LogisticRegression => Accuracy: {acc_lr:.4f}, Time: {lr_time:.2f}s\")\n",
        "\n",
        "print(beta_cd[1:10])\n",
        "print(beta_0_cd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac4cekmoj9Cy",
        "outputId": "964cfafb-f1f7-4f4d-da34-cbcf1afcbd1a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded. Shape of X: (62, 2000)\n",
            "Epoch 1, Loss=0.463430, ||Δβ||=0.394558\n",
            "Epoch 2, Loss=0.440034, ||Δβ||=0.338407\n",
            "Epoch 3, Loss=0.437141, ||Δβ||=0.376197\n",
            "\n",
            "Custom Logistic CD => Accuracy: 0.6923, Time: 18.86s\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "-1.046029373064092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tolerance based stoppage"
      ],
      "metadata": {
        "id": "bDKaUmymrnzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_cd_fast(\n",
        "    X, y, beta, beta_0, s, max_iter=2000, tol=1e-4,\n",
        "    lambda_para=0.2, alpha_para=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Logistic Coordinate Descent with s-step block updates (elastic net regularization).\n",
        "    \"\"\"\n",
        "    # Flatten beta to shape (p,) for easier indexing\n",
        "    if beta.ndim == 2 and beta.shape[1] == 1:\n",
        "        beta = beta.flatten()\n",
        "\n",
        "    n, p = X.shape\n",
        "    epsilon = 1e-8\n",
        "    epoch = 0\n",
        "    beta_old = beta.copy()\n",
        "\n",
        "    # We'll do max_iter passes, each pass ~ p/s block-updates\n",
        "    total_iter = max_iter * int(np.ceil(p / s))\n",
        "\n",
        "    # Helper to compute logistic loss w/ elastic net penalty\n",
        "    def logistic_loss(y_true, y_prob, coef):\n",
        "        eps = 1e-8\n",
        "        # Negative log-likelihood\n",
        "        neg_ll = -(\n",
        "            y_true @ np.log(y_prob + eps) +\n",
        "            (1 - y_true) @ np.log(1 - y_prob + eps)\n",
        "        ) / n\n",
        "        # Elastic net penalty\n",
        "        l2_term = 0.5 * np.sum(coef**2)\n",
        "        l1_term = np.sum(np.abs(coef))\n",
        "        penalty = lambda_para * ((1 - alpha_para) * l2_term + alpha_para * l1_term)\n",
        "        return neg_ll + penalty\n",
        "\n",
        "    # Coordinate Descent\n",
        "    for iteration in range(total_iter):\n",
        "        # Determine which block of coordinates to update\n",
        "        start = (iteration * s) % p\n",
        "        end = start + s\n",
        "        if end <= p:\n",
        "            sampled_indices = np.arange(start, end)\n",
        "        else:\n",
        "            # Wrap around if needed\n",
        "            sampled_indices = np.concatenate([\n",
        "                np.arange(start, p),\n",
        "                np.arange(0, end % p)\n",
        "            ])\n",
        "\n",
        "        # Check if we've completed one full pass of p features => an \"epoch\"\n",
        "        if end % p == 0 and iteration != 0:\n",
        "            epoch += 1\n",
        "\n",
        "            # Evaluate progress after each epoch\n",
        "            y_prob = expit(beta_0 + X @ beta)\n",
        "            loss_val = logistic_loss(y, y_prob, beta)\n",
        "            diff_norm = np.linalg.norm(beta_old - beta)\n",
        "            print(f\"Epoch {epoch}, Loss={loss_val:.6f}, diff_beta={diff_norm:.6f}\")\n",
        "\n",
        "            # Check for convergence\n",
        "            if diff_norm < tol * np.sqrt(p):\n",
        "                print(f\"Converged at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "            # Update beta_old for the next epoch check\n",
        "            beta_old = beta.copy()\n",
        "\n",
        "        # ----- Update step below -----\n",
        "        # Compute sigmoid & derivative\n",
        "        sig = expit(beta_0 + X @ beta)\n",
        "        W = sig * (1 - sig)\n",
        "\n",
        "        # Update intercept\n",
        "        delta_beta_0 = (np.sum(y) - np.sum(sig)) / (np.sum(W) + epsilon)\n",
        "        beta_0 += delta_beta_0\n",
        "\n",
        "        # Extract the block X_1s\n",
        "        X_1s = X[:, sampled_indices]\n",
        "\n",
        "        # Compute A and B for the chosen block\n",
        "        A = X_1s.T @ sig\n",
        "        B = X_1s.T @ (W[:, None] * X_1s)\n",
        "\n",
        "        # Update coefficients in this block\n",
        "        for l in range(len(sampled_indices)):\n",
        "            idx = sampled_indices[l]\n",
        "            B_ll = B[l, l]\n",
        "            A_l = A[l]\n",
        "\n",
        "            # Summation from previously updated coords in this block\n",
        "            block_indices = sampled_indices[:l]\n",
        "            summation_term = B[l, :l] @ (beta[block_indices] - beta_old[block_indices])\n",
        "\n",
        "            numerator = (\n",
        "                (1.0 / n) * (y @ X_1s[:, l] - A_l - summation_term)\n",
        "                - lambda_para * (1 - alpha_para) * beta[idx]\n",
        "            )\n",
        "            denominator = ((1.0 / n) * B_ll + lambda_para * (1 - alpha_para) + epsilon)\n",
        "            db_l = numerator / denominator\n",
        "            l1_term = lambda_para * alpha_para / denominator\n",
        "\n",
        "            candidate = beta[idx] + db_l\n",
        "            if abs(candidate) <= l1_term:\n",
        "                beta[idx] = 0.0\n",
        "            elif candidate < 0:\n",
        "                beta[idx] = candidate + l1_term\n",
        "            else:\n",
        "                beta[idx] = candidate - l1_term\n",
        "\n",
        "    # Reshape back to (p,1)\n",
        "    return beta.reshape(-1, 1), beta_0"
      ],
      "metadata": {
        "id": "273wtQaWnPww"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Test Script (Runs Directly)\n",
        "###############################################################################\n",
        "\n",
        "# 1. Load data (colon-cancer.txt, for example). Update path if needed:\n",
        "file_path = \"/content/drive/My Drive/s-step-Logistic-Coordinate-Descent/data/colon-cancer.txt\"\n",
        "X_list, y_list = [], []\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        label = int(float(parts[0]))\n",
        "        y_list.append(0 if label == -1 else label)\n",
        "        features = {\n",
        "            int(kv.split(':')[0]): float(kv.split(':')[1])\n",
        "            for kv in parts[1:]\n",
        "        }\n",
        "        X_list.append(features)\n",
        "\n",
        "X = pd.DataFrame(X_list).fillna(0).sort_index(axis=1)\n",
        "y = np.array(y_list)\n",
        "print(\"Data loaded. Shape of X:\", X.shape)\n",
        "\n",
        "# 2. Train/Test Split\n",
        "n_samples = X.shape[0]\n",
        "split_index = int(n_samples * 0.8)\n",
        "X_train = X.iloc[:split_index].values\n",
        "y_train = y[:split_index]\n",
        "X_test  = X.iloc[split_index:].values\n",
        "y_test  = y[split_index:]\n",
        "\n",
        "# 3. Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Initialization\n",
        "n, p = X_train.shape\n",
        "beta_init = np.zeros((p, 1))\n",
        "eps = 1e-12\n",
        "y_mean = np.mean(y_train)\n",
        "beta_0_init = np.log((y_mean + eps) / (1 - y_mean + eps))\n",
        "\n",
        "# 5. Run our logistic CD\n",
        "s = 1024\n",
        "max_iter = 2000\n",
        "start_time_cd = time.time()\n",
        "beta_cd, beta_0_cd = logistic_cd_fast(\n",
        "    X_train, y_train, beta_init, beta_0_init, s,\n",
        "    max_iter=max_iter, tol=1e-4, lambda_para=0.2, alpha_para=0.5\n",
        ")\n",
        "cd_time = time.time() - start_time_cd\n",
        "\n",
        "# 6. Evaluate Performance\n",
        "logits_test_cd = beta_0_cd + X_test @ beta_cd\n",
        "y_prob_cd = expit(logits_test_cd).ravel()\n",
        "y_pred_cd = (y_prob_cd >= 0.5).astype(int)\n",
        "acc_cd = accuracy_score(y_test, y_pred_cd)\n",
        "\n",
        "print(f\"\\nCustom Logistic CD => Accuracy: {acc_cd:.4f}, Time: {cd_time:.2f}s\")\n",
        "\n",
        "\n",
        "print(beta_cd[1:10])\n",
        "print(beta_0_cd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-sqmEFyrqYV",
        "outputId": "97e35e89-3255-43df-9910-74fc627d81c4"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded. Shape of X: (62, 2000)\n",
            "Epoch 1, Loss=0.463430, diff_beta=0.394558\n",
            "Epoch 2, Loss=0.440034, diff_beta=0.338407\n",
            "Epoch 3, Loss=0.437141, diff_beta=0.376197\n",
            "Epoch 4, Loss=0.432828, diff_beta=0.373986\n",
            "Epoch 5, Loss=0.431867, diff_beta=0.368230\n",
            "Epoch 6, Loss=0.429125, diff_beta=0.327126\n",
            "Epoch 7, Loss=0.428157, diff_beta=0.235275\n",
            "Epoch 8, Loss=0.428039, diff_beta=0.212990\n",
            "Epoch 9, Loss=0.427304, diff_beta=0.175854\n",
            "Epoch 10, Loss=0.427604, diff_beta=0.180247\n",
            "Epoch 11, Loss=0.427750, diff_beta=0.156974\n",
            "Epoch 12, Loss=0.428478, diff_beta=0.159803\n",
            "Epoch 13, Loss=0.427320, diff_beta=0.117853\n",
            "Epoch 14, Loss=0.425337, diff_beta=0.111355\n",
            "Epoch 15, Loss=0.424723, diff_beta=0.090813\n",
            "Epoch 16, Loss=0.424619, diff_beta=0.072253\n",
            "Epoch 17, Loss=0.424634, diff_beta=0.062932\n",
            "Epoch 18, Loss=0.424661, diff_beta=0.058913\n",
            "Epoch 19, Loss=0.424551, diff_beta=0.047309\n",
            "Epoch 20, Loss=0.424399, diff_beta=0.041513\n",
            "Epoch 21, Loss=0.424297, diff_beta=0.032220\n",
            "Epoch 22, Loss=0.424182, diff_beta=0.028811\n",
            "Epoch 23, Loss=0.424097, diff_beta=0.028865\n",
            "Epoch 24, Loss=0.424051, diff_beta=0.022788\n",
            "Epoch 25, Loss=0.424046, diff_beta=0.022020\n",
            "Epoch 26, Loss=0.424045, diff_beta=0.016725\n",
            "Epoch 27, Loss=0.424036, diff_beta=0.015642\n",
            "Epoch 28, Loss=0.424026, diff_beta=0.012235\n",
            "Epoch 29, Loss=0.424025, diff_beta=0.009070\n",
            "Epoch 30, Loss=0.424025, diff_beta=0.007692\n",
            "Epoch 31, Loss=0.424024, diff_beta=0.005615\n",
            "Epoch 32, Loss=0.424024, diff_beta=0.004815\n",
            "\n",
            "Custom Logistic CD => Accuracy: 0.6923, Time: 140.29s\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "-1.0394697382004043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed Acceleration"
      ],
      "metadata": {
        "id": "srDfZLFWuCjK"
      }
    }
  ]
}